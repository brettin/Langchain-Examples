{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433062aa-1aec-4b5b-a312-f389e544a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d57f092-196e-499c-8d34-2d2290b7a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d872e6a8-eeca-4a67-93d6-18f581ec722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 45.5M/45.5M [00:00<00:00, 98.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at:  /home/brettin/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fbd6ac-7aa9-472b-adfd-cd9f753e28d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b807d195-8bbf-4b43-98cc-94dd150803a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7cddf6-fbdb-4369-b9ee-038fe353b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model\n",
    "### Download a GGML converted model\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e07724-864a-4c37-8370-6b729edbc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87284513-7151-44e4-b518-b2920e72af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 8 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 1: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 2: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 3: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 4: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 5: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 6: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "  Device 7: NVIDIA A100-SXM4-40GB, compute capability 8.0\n",
      "llama.cpp: loading model from /rbscratch/brettin/.cache/llama-2-7b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA A100-SXM4-40GB) as main device\n",
      "llama_model_load_internal: mem required  = 5073.82 MB (+ 1024.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 1 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 1/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 529 MB\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/rbscratch/brettin/.cache/llama-2-7b-chat.ggmlv3.q5_1.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2402f33-1b1f-480c-8ab4-6ede9e24b9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stephen Colbert:\n",
      "Yo, John, you're a funny guy\n",
      "But when it comes to politics, you're as dry as the Sahara\n",
      "You talk about issues with a straight face\n",
      "While I bring the laughs and the heat in this race\n",
      "\n",
      "John Oliver:\n",
      "Oh, Stephen, you think you're"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29476360-4f7a-47c4-9555-e4fa9bd32ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A future demo using a different local model that runs on CPU only\n",
    "# from langchain.llms import GPT4All\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3bd9a-1490-456e-918a-4b681a6db17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an LLMChain (see here) with either model by passing in the retrieved docs and a simple prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19a72b-d3b2-4e45-b8cb-6686ef2b93dd",
   "metadata": {},
   "source": [
    "An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n",
    "\n",
    "An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bc32a-cd12-46b2-b5fc-9f8c5d86d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "result = llm_chain(docs)\n",
    "\n",
    "# Output\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6770d461-ad6c-4642-ba02-1264d2beeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import PubMedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b4f52a-62ce-4cee-9496-012fc34bd587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too Many Requests, waiting for 0.20 seconds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'uid': '37493633', 'Title': {'i': 'Pseudomonas aeruginosa', '#text': 'Molecular determinants of avoidance and inhibition of  MexB efflux pump.'}, 'Published': '2023-07-26', 'Copyright Information': ''}),\n",
       " Document(page_content='', metadata={'uid': '37369638', 'Title': 'iAMPCN: a deep-learning approach for identifying antimicrobial peptides and their functional activities.', 'Published': '--', 'Copyright Information': '© The Author(s) 2023. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.'}),\n",
       " Document(page_content='', metadata={'uid': '37368803', 'Title': 'DeepTPpred: A Deep Learning Approach with Matrix Factorization for Predicting Therapeutic Peptides by Integrating Length Information.', 'Published': '2023-06-27', 'Copyright Information': ''})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = PubMedRetriever(doc_content_chars_max=10000)\n",
    "retriever.get_relevant_documents(\"deep learning approaches to antibiotic discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe4137-cd05-4d4b-a2a6-33f603d36023",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
