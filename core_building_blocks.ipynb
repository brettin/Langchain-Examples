{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13514ce9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The core building block of a LangChain application is the LLMChain. This combines three things:\n",
    "\n",
    "* LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
    "\n",
    "* Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
    "\n",
    "* Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54db0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c95c89",
   "metadata": {},
   "source": [
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an (optional) output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f17e5",
   "metadata": {},
   "source": [
    "## The local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37a0955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /rbscratch/brettin/.cache/llama-2-7b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA A100-SXM4-40GB) as main device\n",
      "llama_model_load_internal: mem required  = 5073.82 MB (+ 1024.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 1 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 1/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 529 MB\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    }
   ],
   "source": [
    "# These just enables streaming of the output tokens as they are predicted\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# These are paramters for LlamaCpp. Different models will have different paramters.\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Create an llm\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/rbscratch/brettin/.cache/llama-2-7b-chat.ggmlv3.q5_1.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    #callback_manager=callback_manager,\n",
    "    #verbose=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513604c4",
   "metadata": {},
   "source": [
    "## The prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b3bcb",
   "metadata": {},
   "source": [
    "### Example 1 (This one seems convoluted to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c25dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f33be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assistant: Sure! Here are 5 types of cancer:\\nBreast Cancer',\n",
       " 'Lung Cancer',\n",
       " 'Colorectal Cancer',\n",
       " 'Prostate Cancer',\n",
       " 'Skin Cancer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n",
    "chain.run(\"cancers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ba1d3",
   "metadata": {},
   "source": [
    "### Example 2 (This seems more direct to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d58a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more. The category is {category}\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"category\",]\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f234ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'trains',\n",
       " 'or automobiles. Please answer the following:\\nWhat are five types of airplanes?\\nWhat are five types of trains?\\nWhat are five types of cars?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n",
    "chain.run(\"planes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35006338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334688ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
